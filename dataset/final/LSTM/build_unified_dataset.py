#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LSTM daily ‚Üí weekly ‚Üí monthly (sync with GNN windows) ‚Äî ultra low‚Äëmem (‚â§12 GB).

–í—Ö–æ–¥:
  /mnt/d/new_Fraud/dataset/final/LSTM/parquet/*.parquet
  /mnt/d/new_Fraud/dataset/final/gnn_dataset/labels/targets_global.parquet
  /mnt/d/new_Fraud/dataset/final/gnn_dataset/mapping/address_id_map_labels.parquet
  /mnt/d/new_Fraud/dataset/final/gnn_dataset/meta/week_window_meta.parquet
  /mnt/d/new_Fraud/dataset/final/gnn_dataset/meta/month_window_meta.parquet

–í—ã—Ö–æ–¥ (–æ—Ç–¥–µ–ª—å–Ω–∞—è –ø–∞–ø–∫–∞, –ù–ï gnn_dataset):
  /mnt/d/new_Fraud/dataset/final/lstm_dataset/
    ‚îú‚îÄ daily_filtered.parquet
    ‚îú‚îÄ weekly.parquet
    ‚îú‚îÄ monthly.parquet
    ‚îî‚îÄ README.md
"""

from __future__ import annotations
from pathlib import Path
import os
import polars as pl

# ---- 12 GB friendly ----
os.environ["POLARS_MAX_THREADS"] = os.environ.get("POLARS_MAX_THREADS", "2")
ROW_GROUP = 128_000  # –º–∞–ª–µ–Ω—å–∫–∏–µ row-groups —Å–Ω–∏–∂–∞—é—Ç –ø–∏–∫–æ–≤—É—é –ø–∞–º—è—Ç—å –ø—Ä–∏ –∑–∞–ø–∏—Å–∏

BASE = Path("/mnt/d/new_Fraud/dataset/final")
SRC_LSTM_DAILY = BASE / "LSTM" / "parquet"         # —Ç–≤–æ–∏ –¥–Ω–µ–≤–Ω—ã–µ parquet-—Ñ–∞–π–ª—ã

# –æ–±—â–∏–π –Ω–∞–±–æ—Ä –¥–ª—è GNN (labels/meta)
DS_ROOT = BASE / "gnn_dataset"
# –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–æ—Ä–µ–Ω—å LSTM-–≤—ã—Ö–æ–¥–æ–≤
OUT_DIR = BASE / "lstm_dataset"
OUT_DIR.mkdir(parents=True, exist_ok=True)

LABELS_FP = DS_ROOT / "labels" / "targets_global.parquet"
MAP_FP = DS_ROOT / "mapping" / "address_id_map_labels.parquet"
WEEK_META = DS_ROOT / "meta" / "week_window_meta.parquet"
MONTH_META = DS_ROOT / "meta" / "month_window_meta.parquet"

DAILY_OUT = OUT_DIR / "daily_filtered.parquet"
WEEKLY_OUT = OUT_DIR / "weekly.parquet"
MONTHLY_OUT = OUT_DIR / "monthly.parquet"
README_FP = OUT_DIR / "README.md"

TMP_WEEK_DIR = OUT_DIR / "_tmp_week_parts"
TMP_MONTH_DIR = OUT_DIR / "_tmp_month_parts"
TMP_WEEK_DIR.mkdir(parents=True, exist_ok=True)
TMP_MONTH_DIR.mkdir(parents=True, exist_ok=True)


def collect_streaming(lf: pl.LazyFrame) -> pl.DataFrame:
    try:
        return lf.collect(engine="streaming")
    except TypeError:
        return lf.collect()


# ---- –ø–æ–ª—è ----
ETH_NUMS = [
    "eth_sent_sum", "eth_recv_sum", "eth_net_flow",
    "tx_fee_eth_sum", "internal_out_value_eth_sum", "internal_in_value_eth_sum",
]
INT_COUNTS = [
    "normal_sent_cnt", "normal_recv_cnt", "normal_total_cnt", "normal_failed_cnt",
    "normal_to_contract_cnt", "normal_to_eoa_cnt",
    "gas_used_sum",
    "erc20_sent_cnt", "erc20_recv_cnt", "erc20_total_cnt",
    "erc20_unique_tokens_sent", "erc20_unique_tokens_recv",
    "internal_out_cnt", "internal_in_cnt",
    "uniq_peers_cnt", "uniq_contract_peers_cnt", "uniq_eoa_peers_cnt",
    "sessions_cnt",
    "active_span_min",
    "burst_max_tx_5m",  # MAX over window
]
MAX_ONLY = {"burst_max_tx_5m"}
SUM_ONLY = (set(INT_COUNTS) - MAX_ONLY) | set(ETH_NUMS)

# ============== 1) targets + mapping ==============
if not LABELS_FP.exists() or not MAP_FP.exists():
    raise FileNotFoundError(
        "–ù–µ –Ω–∞–π–¥–µ–Ω—ã labels/mapping –≤ gnn_dataset. –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏ GNN gnn_dataset.")
targets = pl.read_parquet(LABELS_FP).select(
    ["node_id", "address"]).with_columns(pl.col("address").str.to_lowercase())
addr_map = pl.read_parquet(MAP_FP).with_columns(
    pl.col("address").str.to_lowercase()).unique(subset=["address"])

# ============== 2) daily ‚Üí filtered + node_id + week/month (streaming) ==============
if not any(SRC_LSTM_DAILY.glob("*.parquet")):
    raise FileNotFoundError(
        f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤—Ö–æ–¥–Ω—ã–µ daily parquet –≤ {SRC_LSTM_DAILY}")

# –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –Ω–µ –¥–µ—Ä–≥–∞—è –ø–æ–ª–Ω—É—é —Å—Ö–µ–º—É (–±–µ–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è)
src_scan = pl.scan_parquet(str(SRC_LSTM_DAILY / "*.parquet"))
src_schema = src_scan.collect_schema()
needed = ["address", "day"] + \
    [c for c in (INT_COUNTS + ETH_NUMS) if c in src_schema]
lf_daily = (
    src_scan
    .select(needed)
    .with_columns(pl.col("address").str.to_lowercase())
    # —Ç–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—ã–µ –∞–¥—Ä–µ—Å–∞; –¥–æ–±–∞–≤–∏—Ç node_id
    .join(addr_map.lazy(), on="address", how="inner")
    .with_columns([
        pl.col("day").cast(pl.Date),
        pl.col("day").dt.strftime("%G-W%V").alias("week"),
        pl.col("day").dt.strftime("%Y-%m").alias("month"),
    ])
    .select(["node_id", "address", "day", "week", "month"] + [c for c in needed if c not in {"address", "day"}])
)
lf_daily.sink_parquet(str(DAILY_OUT), compression="zstd", statistics=True)
print(f"[‚úì] daily_filtered ‚Üí {DAILY_OUT}")

# ============== 3) weekly ‚Äî –ü–û –û–ö–ù–ê–ú (–Ω–∏–∑–∫–∞—è –ø–∞–º—è—Ç—å) ==============
if not WEEK_META.exists():
    raise FileNotFoundError(WEEK_META)
weeks_list = pl.read_parquet(WEEK_META).select("week").to_series().to_list()

# –∞–≥—Ä–µ–≥–∞—Ü–∏–∏


def sum_int(c): return pl.col(c).cast(
    pl.Int64, strict=False).fill_null(0).sum().alias(c)


def max_int(c): return pl.col(c).cast(
    pl.Int64, strict=False).fill_null(0).max().alias(c)


def sum_dec_str(c): return pl.col(c).cast(pl.Decimal(38, 9),
                                          strict=False).fill_null(0).sum().cast(pl.Utf8).alias(c)


weekly_aggs = []
for c in SUM_ONLY:
    if c in ETH_NUMS:
        weekly_aggs.append(sum_dec_str(c))
    else:
        weekly_aggs.append(sum_int(c))
for c in MAX_ONLY:
    weekly_aggs.append(max_int(c))

# —Å—á–∏—Ç–∞–µ–º –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é –æ—Ç–¥–µ–ª—å–Ω–æ –∏ –ø–∏—à–µ–º part-—Ñ–∞–π–ª—ã
for i, w in enumerate(weeks_list, 1):
    part_fp = TMP_WEEK_DIR / f"week={w}.parquet"
    if part_fp.exists():
        if i % 50 == 0:
            print(f"[weekly] skip {i}/{len(weeks_list)}")
        continue
    lf_part = (
        pl.scan_parquet(str(DAILY_OUT))
          .filter(pl.col("week") == w)
          .group_by(["node_id", "week"])
          .agg(weekly_aggs)
          .select(["node_id", "week"] + sorted(SUM_ONLY | MAX_ONLY))
    )
    # –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ—Ä–µ–π–º ‚Üí –ø–∏—à–µ–º –Ω–∞–ø—Ä—è–º—É—é
    collect_streaming(lf_part).write_parquet(
        part_fp, compression="zstd", statistics=True, row_group_size=ROW_GROUP)
    if i % 50 == 0:
        print(f"[weekly] {i}/{len(weeks_list)}")

# –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ weekly-–ø–∞—Ä—Ç—ã –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º
parts = sorted(TMP_WEEK_DIR.glob("week=*.parquet"))
if parts:
    pl.scan_parquet([str(p) for p in parts]).sink_parquet(
        str(WEEKLY_OUT), compression="zstd", statistics=True)
print(f"[‚úì] weekly ‚Üí {WEEKLY_OUT}")

# ============== 4) month ‚Äî –ò–ó –ù–ï–î–ï–õ–¨ (–Ω–∏–∑–∫–∞—è –ø–∞–º—è—Ç—å) ==============
if not MONTH_META.exists():
    raise FileNotFoundError(MONTH_META)

# –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (week ‚Üí month) –∏–∑ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ daily_filtered
week_month_map = (
    pl.scan_parquet(str(DAILY_OUT))
      .select(["week", "month"])
      .unique()
      .collect()     # —ç—Ç–æ –º–∞–ª–µ–Ω—å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞ (–ø–æ —á–∏—Å–ª—É –Ω–µ–¥–µ–ª—å)
)

months_list = pl.read_parquet(MONTH_META).select("month").to_series().to_list()

# –ü—Ä–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ –º–µ—Å—è—Ü–∞–º:


def sum_int_m(c): return pl.col(c).cast(
    pl.Int64, strict=False).fill_null(0).sum().alias(c)


def sum_dec_str_m(c): return pl.col(c).cast(pl.Utf8).cast(
    pl.Decimal(38, 9), strict=False).fill_null(0).sum().cast(pl.Utf8).alias(c)


def max_int_m(c): return pl.col(c).cast(
    pl.Int64, strict=False).fill_null(0).max().alias(c)


monthly_aggs = []
for c in SUM_ONLY:
    if c in ETH_NUMS:
        # weekly-–∑–Ω–∞—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∫–æ–≤—ã–µ ‚Üí –æ–±—Ä–∞—Ç–Ω–æ –≤ Decimal ‚Üí sum ‚Üí —Å—Ç—Ä–æ–∫–∞
        monthly_aggs.append(sum_dec_str_m(c))
    else:
        monthly_aggs.append(sum_int_m(c))
for c in MAX_ONLY:
    monthly_aggs.append(max_int_m(c))

for i, m in enumerate(months_list, 1):
    part_fp = TMP_MONTH_DIR / f"month={m}.parquet"
    if part_fp.exists():
        if i % 25 == 0:
            print(f"[monthly] skip {i}/{len(months_list)}")
        continue

    # –Ω–µ–¥–µ–ª–∏, –≤—Ö–æ–¥—è—â–∏–µ –≤ –º–µ—Å—è—Ü m
    w_in_m = week_month_map.filter(pl.col("month") == m).select(
        "week").to_series().to_list()
    if not w_in_m:
        continue

    lf_m = (
        pl.scan_parquet(str(WEEKLY_OUT))
          .filter(pl.col("week").is_in(w_in_m))
          .group_by("node_id")
          .agg(monthly_aggs)
          .with_columns(pl.lit(m).alias("month"))
          .select(["node_id", "month"] + sorted(SUM_ONLY | MAX_ONLY))
    )
    collect_streaming(lf_m).write_parquet(
        part_fp, compression="zstd", statistics=True, row_group_size=ROW_GROUP)
    if i % 25 == 0:
        print(f"[monthly] {i}/{len(months_list)}")

# –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ monthly-–ø–∞—Ä—Ç—ã –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
parts_m = sorted(TMP_MONTH_DIR.glob("month=*.parquet"))
if parts_m:
    pl.scan_parquet([str(p) for p in parts_m]).sink_parquet(
        str(MONTHLY_OUT), compression="zstd", statistics=True)
print(f"[‚úì] monthly ‚Üí {MONTHLY_OUT}")

# ============== 5) README.md ==============
README_FP.write_text(
    "# üìä Fraud LSTM Dataset\n\n"
    "–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LSTM, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω —Å GNN (`gnn_dataset`) –ø–æ –∞–¥—Ä–µ—Å–∞–º (`node_id`) –∏ –æ–∫–Ω–∞–º (`week`, `month`).\n\n"
    "## –§–∞–π–ª—ã\n"
    "- `daily_filtered.parquet` ‚Äî –¥–Ω–µ–≤–Ω—ã–µ —Ñ–∏—á–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –∞–¥—Ä–µ—Å–æ–≤, —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ `node_id`, `address`, `day`, `week`, `month`.\n"
    "- `weekly.parquet` ‚Äî –∞–≥—Ä–µ–≥–∞—Ç—ã –ø–æ ISO‚Äë–Ω–µ–¥–µ–ª—è–º (YYYY-Www).\n"
    "- `monthly.parquet` ‚Äî –∞–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –º–µ—Å—è—Ü–∞–º (YYYY-MM), —Å–æ–±—Ä–∞–Ω—ã **–∏–∑ –Ω–µ–¥–µ–ª—å**.\n\n"
    "## –ê–≥—Ä–µ–≥–∞—Ü–∏–∏\n"
    "- –í—Å–µ —Å—á—ë—Ç—á–∏–∫–∏ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è.\n"
    "- ETH‚Äë–ø–æ–ª—è —Å—É–º–º–∏—Ä—É—é—Ç—Å—è –∫–∞–∫ Decimal(38,9) –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Å—Ç—Ä–æ–∫–æ–π (–±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏).\n"
    "- `burst_max_tx_5m` ‚Äî –±–µ—Ä—ë–º –º–∞–∫—Å–∏–º—É–º –≤ –æ–∫–Ω–µ.\n\n"
    "## –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è\n"
    "- –°–æ–≤–ø–∞–¥–∞—é—â–∏–µ `node_id` —Å `gnn_dataset/labels/targets_global.parquet`.\n"
    "- –°–æ–≤–ø–∞–¥–∞—é—â–∏–µ –æ–∫–Ω–∞ —Å `gnn_dataset/meta/week_window_meta.parquet` –∏ `month_window_meta.parquet`.\n",
    encoding="utf-8"
)
print(f"[‚úì] README ‚Üí {README_FP}")

print(f"\n[‚úì] LSTM dataset ready ‚Üí {OUT_DIR}\n"
      f"    weekly parts:  {TMP_WEEK_DIR}\n"
      f"    monthly parts: {TMP_MONTH_DIR}\n")
